{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similar_keyphrases(input, m_kav_mean):    \n",
    "    cs = cosine_similarity(m_kav_mean[input].reshape(1, -1),m_kav_mean)[0]\n",
    "    cands = cs.argsort()[::-1][1:6]\n",
    "    return cands\n",
    "\n",
    "def get_similar_keyphrases_movies(input, m_kav_mean, m_items):\n",
    "    cs = cosine_similarity(m_kav_mean[input].reshape(1, -1),m_items)[0]\n",
    "    cands = cs.argsort()[::-1][:5]\n",
    "    return cands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = pd.read_csv('./data/yelp/fold0/tr_tags.csv')\n",
    "\n",
    "rows, cols = df_tags.item, df_tags.tag\n",
    "values = np.ones(len(df_tags))\n",
    "\n",
    "m_item_keyphrase= sp.csr_matrix((values, (rows, cols)), dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/yelp/tag_id_dict.json') as f:\n",
    "    tag_id_dict = json.load(f)\n",
    "with open('./data/yelp/id_business_dict.json') as f:\n",
    "    id_business_dict = json.load(f)\n",
    "\n",
    "id_tag_dict = {id: tag for tag, id in tag_id_dict.items()}\n",
    "id_business_dict = {int(id): b for id, b in id_business_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f786850e387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saves/yelp/VAE_beta_multilayer.pt'\n",
    "model = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "item_embeddings = model.decoder.weight.detach().numpy()\n",
    "item_bias = model.decoder.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.07911904633045197"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(item_bias,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4997, 100)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0%|          | 1/245 [00:00<00:25,  9.46it/s]Generate Keyphrase Activation Vector\n100%|██████████| 245/245 [00:18<00:00, 13.28it/s](245, 100, 100)\n\n"
    }
   ],
   "source": [
    "from utils.KAVgenerator import KAVgenerator\n",
    "k = KAVgenerator(m_item_keyphrase,item_embeddings, 20)\n",
    "keyphrase_embeddings = k.get_all_mean_kav(20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.6559366"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.linalg.norm(item_embeddings, ord=2, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.5629330642634401"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.linalg.norm(keyphrase_embeddings, ord=2, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'tea',\n 1: 'parking',\n 2: 'cookie',\n 3: 'milk',\n 4: 'quick',\n 5: 'pricey',\n 6: 'bar',\n 7: 'coffee',\n 8: 'green tea',\n 9: 'convenient',\n 10: 'fancy',\n 11: 'chocolate',\n 12: 'banana',\n 13: 'lemon',\n 14: 'milk tea',\n 15: 'busy',\n 16: 'fresh',\n 17: 'friendly',\n 18: 'quiet',\n 19: 'music',\n 20: 'chip',\n 21: 'baked',\n 22: 'ambiance',\n 23: 'sandwich',\n 24: 'latte',\n 25: 'window',\n 26: 'cozy',\n 27: 'decent price',\n 28: 'flavorful',\n 29: 'coconut',\n 30: 'fried',\n 31: 'fish',\n 32: 'fry',\n 33: 'shrimp',\n 34: 'scallop',\n 35: 'calamari',\n 36: 'dark',\n 37: 'deep fried',\n 38: 'efficient',\n 39: 'classic',\n 40: 'taco',\n 41: 'lettuce',\n 42: 'corn',\n 43: 'tomato',\n 44: 'fair',\n 45: 'sour',\n 46: 'soup',\n 47: 'salad',\n 48: 'crispy',\n 49: 'seafood',\n 50: 'meat',\n 51: 'lobster',\n 52: 'bun',\n 53: 'mayo',\n 54: 'salt',\n 55: 'juicy',\n 56: 'fast',\n 57: 'wait',\n 58: 'stuffed',\n 59: 'yummy',\n 60: 'cake',\n 61: 'squid',\n 62: 'ice cream',\n 63: 'clean',\n 64: 'juice',\n 65: 'honey',\n 66: 'smooth',\n 67: 'stick',\n 68: 'creamy',\n 69: 'dessert',\n 70: 'sweetness',\n 71: 'espresso',\n 72: 'mango',\n 73: 'store',\n 74: 'expensive',\n 75: 'grand opening',\n 76: 'chair',\n 77: 'greeted',\n 78: 'bright',\n 79: 'fluffy',\n 80: 'dinner',\n 81: 'chewy',\n 82: 'fruit',\n 83: 'pleasant',\n 84: 'topped',\n 85: 'refreshing',\n 86: 'spacious',\n 87: 'crunchy',\n 88: 'cheap',\n 89: 'frozen',\n 90: 'apple',\n 91: 'egg',\n 92: 'waffle',\n 93: 'honestly',\n 94: 'overpriced',\n 95: 'washroom',\n 96: 'reasonable',\n 97: 'nicely',\n 98: 'cone',\n 99: 'tax',\n 100: 'casual',\n 101: 'helpful',\n 102: 'chicken',\n 103: 'soggy',\n 104: 'traditional',\n 105: 'fried chicken',\n 106: 'market',\n 107: 'spicy',\n 108: 'cheese',\n 109: 'pop',\n 110: 'downtown',\n 111: 'greasy',\n 112: 'seasoned',\n 113: 'mall',\n 114: 'bubble',\n 115: 'wing',\n 116: 'bubble tea',\n 117: 'lunch',\n 118: 'four',\n 119: 'solid',\n 120: 'asian',\n 121: 'complaint',\n 122: 'noodle',\n 123: 'booth',\n 124: 'chili',\n 125: 'disappointing',\n 126: 'dip',\n 127: 'donut',\n 128: 'french',\n 129: 'attentive',\n 130: 'bacon',\n 131: 'immediately',\n 132: 'rare',\n 133: 'modern',\n 134: 'strawberry',\n 135: 'toast',\n 136: 'station',\n 137: 'potato',\n 138: 'tart',\n 139: 'bean',\n 140: 'burger',\n 141: 'cheaper',\n 142: 'brunch',\n 143: 'bakery',\n 144: 'healthy',\n 145: 'crust',\n 146: 'bread',\n 147: 'high price',\n 148: 'pot',\n 149: 'croissant',\n 150: 'reasonable price',\n 151: 'comfortable',\n 152: 'takeout',\n 153: 'vietnamese',\n 154: 'steamed',\n 155: 'olive',\n 156: 'chinese',\n 157: 'ramen',\n 158: 'crowded',\n 159: 'thai',\n 160: 'beef',\n 161: 'lamb',\n 162: 'rib',\n 163: 'breakfast',\n 164: 'pork',\n 165: 'north york',\n 166: 'japanese',\n 167: 'markham',\n 168: 'skewer',\n 169: 'tofu',\n 170: 'vegetarian',\n 171: 'vegan',\n 172: 'pasta',\n 173: 'italian',\n 174: 'sausage',\n 175: 'wine',\n 176: 'duck',\n 177: 'theatre',\n 178: 'salmon',\n 179: 'smoked',\n 180: 'steak',\n 181: 'good price',\n 182: 'roasted',\n 183: 'birthday',\n 184: 'cocktail',\n 185: 'pizza',\n 186: 'pricy',\n 187: 'refill',\n 188: 'great price',\n 189: 'beer',\n 190: 'cheesecake',\n 191: 'belly',\n 192: 'sushi',\n 193: 'tempura',\n 194: 'miso',\n 195: 'avocado',\n 196: 'sashimi',\n 197: 'tuna',\n 198: 'indian',\n 199: 'curry',\n 200: 'gravy',\n 201: 'buffet',\n 202: 'pub',\n 203: 'bbq',\n 204: 'plaza',\n 205: 'fried rice',\n 206: 'dumpling',\n 207: 'wrap',\n 208: 'mexican',\n 209: 'pancake',\n 210: 'english muffin',\n 211: 'kimchi',\n 212: 'sesame',\n 213: 'pork belly',\n 214: 'oyster',\n 215: 'good vibe',\n 216: 'movie',\n 217: 'shopping',\n 218: 'patty',\n 219: 'dog',\n 220: 'poutine',\n 221: 'uber eats',\n 222: 'spring roll',\n 223: 'octopus',\n 224: 'dim sum',\n 225: 'regular price',\n 226: 'congee',\n 227: 'matcha',\n 228: 'gelato',\n 229: 'dietary restriction',\n 230: 'accept debit',\n 231: 'tapioca',\n 232: 'wild boar',\n 233: 'crepe',\n 234: 'nail',\n 235: 'alcoholic beverage',\n 236: 'lactose intolerant',\n 237: 'financial district',\n 238: 'balsamic vinegar',\n 239: 'pork bone soup',\n 240: 'public transit',\n 241: 'gong cha',\n 242: 'general tao',\n 243: 'pale ale',\n 244: 'mexico'}"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tag_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['milk', 'milk tea', 'bubble', 'noodle', 'soup']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['tea']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['tea'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['cake', 'wing', 'lobster', 'dessert', 'dim sum']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['cheesecake']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['cheesecake'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['fry', 'breakfast', 'pizza', 'pasta', 'juicy']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['burger']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['burger'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['cocktail', 'chip', 'bread', 'octopus', 'mexican']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['taco']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['taco'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['pasta', 'pizza', 'fresh', 'friendly', 'tomato']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['italian']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['italian'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['friendly', 'coffee', 'latte', 'cocktail', 'spicy']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['sandwich']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['sandwich'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['salmon', 'miso', 'creamy', 'green tea', 'fish']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['japanese']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['japanese'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['fresh', 'crispy', 'markham', 'store', 'sesame']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['parking']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['friendly', 'meat', 'fish', 'tuna', 'fresh']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['spicy']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['spicy'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['soup', 'noodle', 'bubble', 'dim sum', 'bubble tea']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['ramen']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['ramen'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['vegan', 'wrap', 'coffee', 'cozy', 'brunch']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['vegetarian']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['vegetarian'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['salad', 'coffee', 'cake', 'potato', 'chocolate']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['healthy']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['healthy'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['bread', 'brunch', 'olive', 'toast', 'wine']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['french']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['french'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['takeout', 'pork', 'noodle', 'pot', 'spring roll']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['cheap']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['cheap'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['meat', 'pricey', 'mayo', 'fresh', 'modern']\n"
    }
   ],
   "source": [
    "id = tag_id_dict['seafood']\n",
    "cands = get_similar_keyphrases(id, keyphrase_embeddings)\n",
    "output = []\n",
    "for kp in cands:\n",
    "    output.append(id_tag_dict[kp])\n",
    "print(output)\n",
    "outputs.append(['seafood'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tea</th>\n      <td>milk</td>\n      <td>milk tea</td>\n      <td>bubble</td>\n      <td>noodle</td>\n      <td>soup</td>\n    </tr>\n    <tr>\n      <th>cheesecake</th>\n      <td>cake</td>\n      <td>wing</td>\n      <td>lobster</td>\n      <td>dessert</td>\n      <td>dim sum</td>\n    </tr>\n    <tr>\n      <th>burger</th>\n      <td>fry</td>\n      <td>breakfast</td>\n      <td>pizza</td>\n      <td>pasta</td>\n      <td>juicy</td>\n    </tr>\n    <tr>\n      <th>taco</th>\n      <td>cocktail</td>\n      <td>chip</td>\n      <td>bread</td>\n      <td>octopus</td>\n      <td>mexican</td>\n    </tr>\n    <tr>\n      <th>italian</th>\n      <td>pasta</td>\n      <td>pizza</td>\n      <td>fresh</td>\n      <td>friendly</td>\n      <td>tomato</td>\n    </tr>\n    <tr>\n      <th>sandwich</th>\n      <td>friendly</td>\n      <td>coffee</td>\n      <td>latte</td>\n      <td>cocktail</td>\n      <td>spicy</td>\n    </tr>\n    <tr>\n      <th>japanese</th>\n      <td>salmon</td>\n      <td>miso</td>\n      <td>creamy</td>\n      <td>green tea</td>\n      <td>fish</td>\n    </tr>\n    <tr>\n      <th>spicy</th>\n      <td>friendly</td>\n      <td>meat</td>\n      <td>fish</td>\n      <td>tuna</td>\n      <td>fresh</td>\n    </tr>\n    <tr>\n      <th>ramen</th>\n      <td>soup</td>\n      <td>noodle</td>\n      <td>bubble</td>\n      <td>dim sum</td>\n      <td>bubble tea</td>\n    </tr>\n    <tr>\n      <th>vegetarian</th>\n      <td>vegan</td>\n      <td>wrap</td>\n      <td>coffee</td>\n      <td>cozy</td>\n      <td>brunch</td>\n    </tr>\n    <tr>\n      <th>healthy</th>\n      <td>salad</td>\n      <td>coffee</td>\n      <td>cake</td>\n      <td>potato</td>\n      <td>chocolate</td>\n    </tr>\n    <tr>\n      <th>french</th>\n      <td>bread</td>\n      <td>brunch</td>\n      <td>olive</td>\n      <td>toast</td>\n      <td>wine</td>\n    </tr>\n    <tr>\n      <th>cheap</th>\n      <td>takeout</td>\n      <td>pork</td>\n      <td>noodle</td>\n      <td>pot</td>\n      <td>spring roll</td>\n    </tr>\n    <tr>\n      <th>seafood</th>\n      <td>meat</td>\n      <td>pricey</td>\n      <td>mayo</td>\n      <td>fresh</td>\n      <td>modern</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                   1          2        3          4            5\ntea             milk   milk tea   bubble     noodle         soup\ncheesecake      cake       wing  lobster    dessert      dim sum\nburger           fry  breakfast    pizza      pasta        juicy\ntaco        cocktail       chip    bread    octopus      mexican\nitalian        pasta      pizza    fresh   friendly       tomato\nsandwich    friendly     coffee    latte   cocktail        spicy\njapanese      salmon       miso   creamy  green tea         fish\nspicy       friendly       meat     fish       tuna        fresh\nramen           soup     noodle   bubble    dim sum   bubble tea\nvegetarian     vegan       wrap   coffee       cozy       brunch\nhealthy        salad     coffee     cake     potato    chocolate\nfrench         bread     brunch    olive      toast         wine\ncheap        takeout       pork   noodle        pot  spring roll\nseafood         meat     pricey     mayo      fresh       modern"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(outputs).set_index(0)\n",
    "df.index.name = None\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Read data from ./data/yelp/fold0/\n"
    }
   ],
   "source": [
    "from utils.Dataset import Dataset\n",
    "dataset = Dataset(data_dir='./data/yelp/fold0/',load_keyphrases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_info(user_id, input_matrix):\n",
    "    user_input = input_matrix[user_id]\n",
    "    nonzeros = user_input.nonzero()\n",
    "    items = nonzeros[1]\n",
    "    ratings = user_input[nonzeros]\n",
    "    ratings = np.asarray(ratings).reshape(-1)\n",
    "    sorted_ratings_idx = ratings.argsort()[::-1]\n",
    "    sorted_items = items[sorted_ratings_idx]\n",
    "    sorted_ratings = ratings[sorted_ratings_idx]\n",
    "    return sorted_items, sorted_ratings\n",
    "\n",
    "def get_user_preds(user_id, input_matrix):\n",
    "    user_input = input_matrix[user_id]\n",
    "    mask_index = user_input.nonzero()[1]\n",
    "    i = torch.FloatTensor(user_input.toarray()).to(torch.device('cpu'))\n",
    "    with torch.no_grad():\n",
    "        preds = model.forward(i).cpu().numpy()\n",
    "    preds = np.asarray(preds).reshape(-1)\n",
    "    preds[mask_index] = -np.inf\n",
    "    sorted_pred_items = preds.argsort()[::-1]\n",
    "    sorted_pred_ratings = preds[sorted_pred_items] \n",
    "    return sorted_pred_items, sorted_pred_ratings\n",
    "\n",
    "def get_mu_cov(user_id, input_matrix, moddel):\n",
    "    user_input = input_matrix[user_id]\n",
    "    i = torch.FloatTensor(user_input.toarray()).to(torch.device('cpu'))\n",
    "    with torch.no_grad():\n",
    "        mu, logvar = model.get_mu_logvar(i)\n",
    "    std = model.logvar2std(logvar)\n",
    "    mu, std = mu.numpy().T, std.numpy()\n",
    "\n",
    "    return mu, np.diagflat(std*std)\n",
    "\n",
    "def get_user_preds_using_mu(user_mu, input_matrix, model):\n",
    "    user_input = input_matrix[user_id]\n",
    "    mask_index = user_input.nonzero()[1]\n",
    "    _mu = torch.FloatTensor(user_mu.T)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model.decoder(_mu)\n",
    "    preds = np.asarray(preds).reshape(-1)\n",
    "    preds[mask_index] = -np.inf\n",
    "    sorted_pred_items = preds.argsort()[::-1]\n",
    "    sorted_pred_ratings = preds[sorted_pred_items] \n",
    "    return sorted_pred_items, sorted_pred_ratings\n",
    "\n",
    "def update_posterior(x, y, S_0, m_0, prec_y):\n",
    "    S_0_inv = np.linalg.inv(S_0)\n",
    "    #S_1 = np.linalg.inv(S_0_inv +prec_y * x @ x.T)    \n",
    "    #print(np.swapaxes(x,-2,-1).shape)\n",
    "    S_1 = np.linalg.inv(S_0_inv +prec_y * np.matmul(x,np.swapaxes(x,-2,-1)))\n",
    "    m_1 = S_1 @ (S_0_inv @ m_0 + prec_y * y * x)\n",
    "    #print(m_1.shape)\n",
    "    return S_1, m_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(7000, 4997)"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "dataset.train_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Gandhi Cuisine', 'Sansotei', 'Naniwa-Taro', 'Athens Pastries', 'One2 Snacks', 'Ghazale', 'Pho Linh', 'Von Doughnuts', 'Stop BBQ Chicken', 'Pho Metro']\n['Fahrenheit Coffee', 'New Toronto Fish & Chips', 'Golden Dough', \"I'll Be Seeing You\", 'Shalom Ethopian Restaurant', \"The Captain's Boil\", 'Noble Coffee', 'Southeast Sandwiches', 'Veghed', 'New Orleans Seafood & Steakhouse', 'Yasu', 'Golden Gecko Coffee', 'Big Cannoli Lane', 'The Alternative Cafe', 'San Antonio Seafood Market', 'Hooked', 'Grill Gate', 'Harvest Green', 'Famiglia Baldassarre', 'The Dumpling Shop', 'Village Juicery', 'Baretto Caffe', 'Pestacio', \"Scheffler's Delicatessen\", 'Cobs Bread', 'Oakwood Espresso', \"Yan's Soy Foods\", 'Food Dudes Pantry', 'Porta Via Restaurant & Catering', 'La Cubana', 'De La Mer', 'Kekou Gelato House', \"Mother's Deli & Bakery\", 'Cakes By Robert', 'Cinnabon', 'Oca Nera', \"Taro's Fish\", 'Amma Roti House', 'Kristapsons', 'Pascale Gourmet', 'Crimson Teas', 'Alo Restaurant', 'La Salumeria', 'Mr. Chestnut', 'Pasta Pantry', 'DAVIDsTEA', 'Mississauga Rotary Ribfest', 'FK', 'Arabesque Middle Eastern Foods', 'Virtuous Pie', 'Momo Hut & Gardens', 'Bulldog Coffee', \"St. Matthew's BBQ Chicken\", 'Plentea', 'Viva Shawarma', 'Patisserie Royale', 'Crosstown Coffee Bar', \"Dante's Inferno Paninoteca\", \"Uncle Mikey's\", 'Nguyên Huong', 'De Floured', 'Grinning Face Non-Dairy Gelato', 'Bodira Cafe and Fine Foods', 'Quetzal', 'Scaramouche Restaurant Pasta Bar & Grill', 'The Coffee Lab', \"Loga's Corner\", 'Sugo', 'Starbucks', 'Green Box Express', 'The Butternut Baking', '9 Bars', 'JP Sushi', 'Melrose On Adelaide', 'Wynona', \"Ruby's Mediterranean Cuisine\", \"Wong's Ice Cream & Store\", 'Ying Ying Soy Food', 'Road Grill Food Truck', 'Panino Cappuccino', 'Bar Sybanne', 'When The Pig Came Home', 'Mystic Muffin', 'Bar Ape', 'Piedmont Coffee Bar', 'The Olde Yorke', 'Loukoumania Café', 'Zeal Burgers', 'JJ Bean', 'Pomarosa Coffee Shop & Kitchen', 'La Paella', 'Baklawa King', 'El Nahual', 'A-Game Cafe', 'Wallace Espresso', 'Kitten and the Bear', \"Gryfe's Bagel Bakery\", 'Adamson Barbecue', 'Orly Grill', \"Kozlik's Canadian Mustard\", 'UB Social Cafe & General Store', 'Pow Wow Cafe', 'Di Manno Bakery', 'Village Meat Products & Deli', 'Ital Vital', 'Taste Seduction', 'Prairie Boy', \"Chica's Nashville Hot Chicken\", 'Burrito Boyz', 'Ay Caramba, Eh', 'Avoca Choclatier', \"Craig's Cookies\", 'La Chilaca Taqueria', 'Tuk Tuk Canteen', 'Starbucks', 'The Shore Leave', 'The Ellery', 'Gloria Espresso Bar & Cafe', 'Tdotjerk', 'Vindaloo Indian Cuisine', 'Cafe Tibet Bar & Grill', 'Ideal Catering', \"Chubby's Subs\", 'Dimpflmeier Bakery', \"Grandmama's Waffles\", \"Blaze Fast-Fire'd Pizza\", 'Zezafoun Syrian Cuisine', 'Aielli Ristorante', 'The Grow Op', 'The Golden Pheasant', 'Asteria Souvlaki Place', 'Bagels On Fire', 'Miku', 'Katsuya', 'La Sem Patisserie', \"Rose's Vietnamese Sandwiches\", \"Rose's Halal Kitchen\", 'Mallo', 'Zav Coffee Shop & Gallery', 'The Sushi Bar', 'Cafe de Melbourne', 'San Antonio Foods', 'The 5th Taste Sushi Restaurant', 'The Corner Bank', 'Lox + Schmear', 'The Epicure Shop', 'Death in Venice Gelato', 'Maison Close 1888', \"Her Father's Cider Bar & Kitchen\", 'The Food Dudes', 'Islas Filipino BBQ and Bar', \"Joso's\", 'Soma Chocolatemaker', 'Le Baratin', 'Pizzeria Defina', 'Grasshopper Restaurant', 'Honest Weight', 'Budapest Restaurant', 'Kekou Gelato', 'Beijing Hot Pot Restaurant', 'Egg Sunrise Grill', 'The Bulging Burger', 'Umami Poke', 'Maple Produce', 'Jugemu', 'Himalayan Kitchen', 'Blooming Orchid', 'Refuel Juicery', 'Futura Granita + Gelato', 'Fragrant Bakery', 'Lageez', 'BB Cafe', 'Millwood Melt Grilled Cheesery', 'The Pie Commission', 'Giulietta', 'Jumbo Burger Restaurant', 'Strange Love', 'Rollz Ice Cream & Desserts', 'Grande Cheese', 'Highland Fish & Chips', 'Famous Last Words', 'Grillway Souvlaki House On Bloor', 'Second Cup', 'Starbucks', \"Bud's Coffee Bar\", 'Bean and Baker Malt Shop', 'Mama Earth Organics', 'Montmartre Bakery', \"Kunafa's\", 'St Clair Ice Cream', 'Pita Golden Pocket', 'Felix & Norton', 'San Francesco Foods - Clinton', 'Sweet Flour Bake Shop', 'Itacate', 'Shoushin', 'VOS Restaurante Argentino', 'Madame Gateaux', 'Rise & Dine Eatery', 'Boxcar Social', 'Crepe TO', 'Mrakovic Meat & Deli', 'Pilot Coffee Roasters', 'Bake Shoppe', 'La Limonada', 'Muncheez', 'Porchetta Roll', 'The Art of Cheese', 'Mazar Kabob', 'The Merseyside', 'Sunny Morning', 'Meat Point', 'Pulp Kitchen', 'Northern Belle', \"Summer's Ice Cream\", \"Allwyn's Bakery\", 'Tandem Coffee', 'Aloette', 'Sasaki Fine Pastry', 'Rasa', 'Takht-e Tavoos', \"Tom's Dairy Freeze\", 'Cheese Boutique', 'Gyubee Japanese BBQ - Downtown', 'Koji Japanese Restaurant', 'Kookminhakgyo', 'Karahi Point', 'Douce France', 'Sushi Wa', 'Tasso Bakery ', 'Mon K Patisserie', 'DOMA', \"Marcheleo's Gourmet Marketplace\", 'Patisserie 27', 'Kay Pacha', 'Hale Coffee', 'Fanzone Wings & Ribs', 'Barans Turkish Cuisine & Bar', 'The Sidekick', 'La Campagna Italian Eatery', 'DAVIDsTEA', 'The Depanneur', 'The Irv Gastro Pub', 'Tropical  Energy Juice Bar', 'The Green Dragon', 'thairoomgrand', 'La Guanaquita', 'Simply Frosted Cupcakery', 'Soul Chocolate', 'Fast Fresh Foods', 'Pantry Foods', 'Local 1794', 'Hodo Kwaja', 'Mare Pizzeria', 'Extra Burger', 'Rikkochez', \"Mike's Fish Market\", 'Creeds Coffee Bar', 'Lake Inez', 'Bake Sale at Six Points', 'Halo Halo', 'Citizenry', 'HotBlack Coffee', 'Fahrenheit Coffee', 'Booster Juice', 'Fabarnak Community Cafe and Catering', 'Zaza Espresso Bar', 'Cocktail Bar', 'Roselle', 'Shawarma Empire', 'Sidebar', 'Cock-A-Doodle-Doo', 'Fruitti Mix', 'Bolan Thai Cuisine', 'Arigato', 'Omaw', 'Silk Restaurant & Bar', 'Artegelato & Caffe', 'Tinuno', 'Starbucks', \"Jordan's Shawarma\", 'The Four Seven', 'Canis', 'Black Camel', \"Ed's Real Scoop\", 'Souv Like', 'Salad Days', \"Mama's Tofu\", 'Pretty Ugly', '2nd Nature Bakery and Cafe', 'Bloor Fruit Market', 'Pegasus On Church', 'Put A Cone On It', \"Mana'ish Global Flatbread Cafe\", 'Imm Thai Kitchen', 'deKEFIR', 'The Bake House', 'Tibet Kitchen', 'Freshouse Juice Bar', \"Nancy's Cheese\", 'Starbucks', 'Blackbird Baking', 'Shawarma Q', 'Coral Sea Fish Market', 'Hailed Coffee', 'Refinery Public House', 'Figs and Olives', 'Civil Liberties', \"V's Caribbean Restaurant\", 'Estia', \"Veggie D'Light\", 'Palmyra Mediterranean House', 'Fearless Meat', 'Delight', 'Daddy O Doughnuts and British Baked Goods', 'Zane Patisserie', 'Sushi Omigoto', 'Cobs Bread', 'Saigon Lotus', 'Founder Bar', \"Ricci's Pizzeria & Sandwich Shoppe\", 'Kings Delight Caribbean Cuisine', 'Royal Jade', 'Rooster Coffee House', 'JJ Bean', 'Ho Sushi Express', 'Noce Restaurant', \"Allan's Pastry Shop\", 'Rooster Coffee House', 'Pizza Break']\n"
    }
   ],
   "source": [
    "user_id = 351\n",
    "sorted_items, sorted_ratings = get_user_info(user_id, dataset.train_matrix)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_business_dict[item_id])\n",
    "print(output)\n",
    "outputs.append(['user history'] + output)\n",
    "\n",
    "sorted_items, sorted_ratings = get_user_preds(user_id, dataset.train_matrix)\n",
    "top10_items = sorted_items[:330]\n",
    "output = []\n",
    "\n",
    "for item_id in top10_items:\n",
    "    output.append(id_business_dict[item_id])\n",
    "print(output)\n",
    "outputs.append(['initial recs'] + output)\n",
    "mu, cov = get_mu_cov(user_id, dataset.train_matrix, model)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4.8263574"
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ratings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "seafood = tag_id_dict['chip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.15784721]])"
     },
     "metadata": {},
     "execution_count": 531
    }
   ],
   "source": [
    "mu.T@keyphrase_embeddings[seafood][:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-2.022886229791352"
     },
     "metadata": {},
     "execution_count": 532
    }
   ],
   "source": [
    "np.quantile(mu.T@(keyphrase_embeddings.T),q=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2.41802716255188"
     },
     "metadata": {},
     "execution_count": 533
    }
   ],
   "source": [
    "np.quantile(mu.T@(item_embeddings.T),q=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(4997, 100)"
     },
     "metadata": {},
     "execution_count": 534
    }
   ],
   "source": [
    "item_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3.2033172233684457 -2.282314772954342\n"
    }
   ],
   "source": [
    "prec = np.linalg.norm(1/(cov+1e-6))\n",
    "neg = np.min(mu.T @ keyphrase_embeddings.T)\n",
    "pos = np.max(mu.T @ keyphrase_embeddings.T)\n",
    "print(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "99498770.0"
     },
     "metadata": {},
     "execution_count": 536
    }
   ],
   "source": [
    "prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = keyphrase_embeddings[seafood][:,np.newaxis]\n",
    "y = [[neg]]\n",
    "\n",
    "cov1, mu1 = update_posterior(x, y, cov, mu, np.array(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.81489264]])"
     },
     "metadata": {},
     "execution_count": 547
    }
   ],
   "source": [
    "mu1.T@keyphrase_embeddings[seafood][:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Fahrenheit Coffee', 'Golden Dough', \"The Captain's Boil\", 'New Toronto Fish & Chips', \"I'll Be Seeing You\", 'Shalom Ethopian Restaurant', 'Noble Coffee', 'Southeast Sandwiches', 'Yasu', 'Veghed', 'Big Cannoli Lane', \"Yan's Soy Foods\", 'The Dumpling Shop', 'Famiglia Baldassarre', 'Golden Gecko Coffee', 'Pestacio', 'New Orleans Seafood & Steakhouse', 'Baretto Caffe', 'Grill Gate', 'Harvest Green', 'San Antonio Seafood Market', 'The Alternative Cafe', 'Cobs Bread', 'Village Juicery', 'Hooked', \"Mother's Deli & Bakery\", 'Oakwood Espresso', \"Scheffler's Delicatessen\", 'Porta Via Restaurant & Catering', 'Kekou Gelato House', 'Amma Roti House', 'Oca Nera', \"Taro's Fish\", 'Food Dudes Pantry', 'De La Mer', 'Kristapsons', 'Cinnabon', 'Cakes By Robert', 'Mr. Chestnut', 'La Cubana', 'Mississauga Rotary Ribfest', 'Alo Restaurant', 'Nguyên Huong', 'Pascale Gourmet', 'Crimson Teas', 'DAVIDsTEA', 'Viva Shawarma', 'Green Box Express', 'Starbucks', \"Dante's Inferno Paninoteca\"]\n"
    }
   ],
   "source": [
    "sorted_items, sorted_ratings = get_user_preds_using_mu(mu1, dataset.train_matrix, model)\n",
    "top10_items = sorted_items[:50]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_business_dict[item_id])\n",
    "print(output)\n",
    "outputs.append(['+ franch'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[\"Yan's Soy Foods\", 'Mr. Chestnut', \"Allan's Pastry Shop\", 'Yasu', 'Grande Cheese', 'The Dumpling Shop', 'Lox + Schmear', 'Fahrenheit Coffee', \"Taro's Fish\", 'Bodira Cafe and Fine Foods']\n"
    }
   ],
   "source": [
    "seafood = tag_id_dict['dessert']\n",
    "x = keyphrase_embeddings[seafood][:,np.newaxis]\n",
    "y = [[pos]]\n",
    "\n",
    "cov2, mu2 = update_posterior(x, y, cov1, mu1, np.array(1000))\n",
    "\n",
    "sorted_items, sorted_ratings = get_user_preds_using_mu(mu2, dataset.train_matrix, model)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_business_dict[item_id])\n",
    "print(output)\n",
    "outputs.append(['- japanese'] + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>user history</th>\n      <td>C &amp; Dubbs Hamburgers</td>\n      <td>Village Ice Cream Shoppe</td>\n      <td>Baretto Caffe</td>\n      <td>The Burger's Priest</td>\n      <td>Mississauga Rotary Ribfest</td>\n      <td>Earls Kitchen + Bar</td>\n      <td>New Orleans Seafood &amp; Steakhouse</td>\n      <td>Pho Hung</td>\n      <td>Primo Veal</td>\n      <td>The Goose &amp; Firkin</td>\n    </tr>\n    <tr>\n      <th>initial recs</th>\n      <td>San Antonio Seafood Market</td>\n      <td>Porchetta Roll</td>\n      <td>Oakwood Espresso</td>\n      <td>Ruby's Mediterranean Cuisine</td>\n      <td>Yasu</td>\n      <td>Rose's Halal Kitchen</td>\n      <td>Lickadee Split</td>\n      <td>Pestacio</td>\n      <td>Adamson Barbecue</td>\n      <td>The Dumpling Shop</td>\n    </tr>\n    <tr>\n      <th>+ japanese</th>\n      <td>Yasu</td>\n      <td>The Dumpling Shop</td>\n      <td>Katsuya</td>\n      <td>Famiglia Baldassarre</td>\n      <td>Loukoumania Café</td>\n      <td>Ruby's Mediterranean Cuisine</td>\n      <td>Oakwood Espresso</td>\n      <td>Porchetta Roll</td>\n      <td>Taro's Fish</td>\n      <td>Adamson Barbecue</td>\n    </tr>\n    <tr>\n      <th>- japanese</th>\n      <td>San Antonio Seafood Market</td>\n      <td>Rose's Halal Kitchen</td>\n      <td>Patty Time</td>\n      <td>The Manna</td>\n      <td>Porchetta Roll</td>\n      <td>Gale's Snack Bar</td>\n      <td>Pestacio</td>\n      <td>Oakwood Espresso</td>\n      <td>Viva Shawarma</td>\n      <td>Karahi Point</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                      1                         2   \\\nuser history        C & Dubbs Hamburgers  Village Ice Cream Shoppe   \ninitial recs  San Antonio Seafood Market            Porchetta Roll   \n+ japanese                          Yasu         The Dumpling Shop   \n- japanese    San Antonio Seafood Market      Rose's Halal Kitchen   \n\n                            3                             4   \\\nuser history     Baretto Caffe           The Burger's Priest   \ninitial recs  Oakwood Espresso  Ruby's Mediterranean Cuisine   \n+ japanese             Katsuya          Famiglia Baldassarre   \n- japanese          Patty Time                     The Manna   \n\n                                      5                             6   \\\nuser history  Mississauga Rotary Ribfest           Earls Kitchen + Bar   \ninitial recs                        Yasu          Rose's Halal Kitchen   \n+ japanese              Loukoumania Café  Ruby's Mediterranean Cuisine   \n- japanese                Porchetta Roll              Gale's Snack Bar   \n\n                                            7                 8   \\\nuser history  New Orleans Seafood & Steakhouse          Pho Hung   \ninitial recs                    Lickadee Split          Pestacio   \n+ japanese                    Oakwood Espresso    Porchetta Roll   \n- japanese                            Pestacio  Oakwood Espresso   \n\n                            9                   10  \nuser history        Primo Veal  The Goose & Firkin  \ninitial recs  Adamson Barbecue   The Dumpling Shop  \n+ japanese         Taro's Fish    Adamson Barbecue  \n- japanese       Viva Shawarma        Karahi Point  "
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(outputs).set_index(0)\n",
    "df.index.name = None\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Pulp Fiction', 'Battleship Potemkin, The (Bronenosets Potyomkin)', 'Woman Under the Influence, A', 'Apocalypse Now', 'Seven Samurai (Shichinin no samurai)', 'Wings of Desire (Der Himmel über Berlin)', 'Son, The (Le Fils)', 'Stroszek', 'Rio Bravo', 'Wallace & Gromit: The Curse of the Were-Rabbit']\n['Pier, The (La Jetée)', '2001: A Space Odyssey', '8 1/2', 'Sunrise: A Song of Two Humans', 'Andrei Rublev (Andrey Rublyov)', 'Gerry', 'Paranoid Park', 'Wild Bunch, The', 'Red River', 'Cyclo (Xich lo)']\n2.684969023457633 -2.7016516940258937\n"
    }
   ],
   "source": [
    "user_id = 300\n",
    "sorted_items, sorted_ratings = get_user_info(user_id, dataset.train_matrix)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_title_dict[item_id][:-7])\n",
    "\n",
    "outputs.append(['user history'] + output)\n",
    "print(output)\n",
    "\n",
    "sorted_items, sorted_ratings = get_user_preds(user_id, dataset.train_matrix)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_title_dict[item_id][:-7])\n",
    "outputs.append(['initial recs'] + output)\n",
    "print(output)\n",
    "\n",
    "mu, cov = get_mu_cov(user_id, dataset.train_matrix, model)\n",
    "prec = np.linalg.norm(1/(cov+1e-6))\n",
    "\n",
    "prec = np.linalg.norm(1/(cov+1e-6))\n",
    "neg = np.min(mu.T @ keyphrase_embeddings.T)\n",
    "pos = np.max(mu.T @ keyphrase_embeddings.T)\n",
    "print(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Pier, The (La Jetée)', '2001: A Space Odyssey', '8 1/2', 'Sunrise: A Song of Two Humans', 'Andrei Rublev (Andrey Rublyov)', 'Gerry', 'Paranoid Park', 'Wild Bunch, The', 'Red River', 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb']\n"
    }
   ],
   "source": [
    "x = keyphrase_embeddings[18][:,np.newaxis]\n",
    "y = [[pos]]\n",
    "\n",
    "cov1, mu1 = update_posterior(x, y, cov, mu, np.array(1))\n",
    "\n",
    "sorted_items, sorted_ratings = get_user_preds_using_mu(mu1, dataset.train_matrix, model)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_title_dict[item_id][:-7])\n",
    "outputs.append(['+ fantasy'] + output)\n",
    "print(output)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Pier, The (La Jetée)', 'Sunrise: A Song of Two Humans', 'Sorrow and the Pity, The (Chagrin et la pitié, Le)', 'Gerry', '2001: A Space Odyssey', 'Paranoid Park', 'Andrei Rublev (Andrey Rublyov)', '8 1/2', 'Au Hasard Balthazar', 'Fat City']\n"
    }
   ],
   "source": [
    "x = keyphrase_embeddings[18][:,np.newaxis]\n",
    "y = [[neg]]\n",
    "\n",
    "cov1, mu1 = update_posterior(x, y, cov, mu, np.array(10))\n",
    "\n",
    "sorted_items, sorted_ratings = get_user_preds_using_mu(mu1, dataset.train_matrix, model)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_title_dict[item_id][:-7])\n",
    "outputs.append(['- fantasy'] + output)\n",
    "print(output)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>user history</th>\n      <td>Pulp Fiction</td>\n      <td>Battleship Potemkin, The (Bronenosets Potyomkin)</td>\n      <td>Woman Under the Influence, A</td>\n      <td>Apocalypse Now</td>\n      <td>Seven Samurai (Shichinin no samurai)</td>\n      <td>Wings of Desire (Der Himmel über Berlin)</td>\n      <td>Son, The (Le Fils)</td>\n      <td>Stroszek</td>\n      <td>Rio Bravo</td>\n      <td>Wallace &amp; Gromit: The Curse of the Were-Rabbit</td>\n    </tr>\n    <tr>\n      <th>initial recs</th>\n      <td>Pier, The (La Jetée)</td>\n      <td>2001: A Space Odyssey</td>\n      <td>8 1/2</td>\n      <td>Sunrise: A Song of Two Humans</td>\n      <td>Andrei Rublev (Andrey Rublyov)</td>\n      <td>Gerry</td>\n      <td>Paranoid Park</td>\n      <td>Wild Bunch, The</td>\n      <td>Red River</td>\n      <td>Cyclo (Xich lo)</td>\n    </tr>\n    <tr>\n      <th>+ fantasy</th>\n      <td>2001: A Space Odyssey</td>\n      <td>Pier, The (La Jetée)</td>\n      <td>8 1/2</td>\n      <td>Dr. Strangelove or: How I Learned to Stop Worr...</td>\n      <td>Wild Bunch, The</td>\n      <td>Touch of Evil</td>\n      <td>Chinatown</td>\n      <td>City Lights</td>\n      <td>Beauty and the Beast (Belle et la bête, La)</td>\n      <td>Andrei Rublev (Andrey Rublyov)</td>\n    </tr>\n    <tr>\n      <th>- fantasy</th>\n      <td>Pier, The (La Jetée)</td>\n      <td>Sunrise: A Song of Two Humans</td>\n      <td>2001: A Space Odyssey</td>\n      <td>8 1/2</td>\n      <td>Gerry</td>\n      <td>Andrei Rublev (Andrey Rublyov)</td>\n      <td>Paranoid Park</td>\n      <td>Sorrow and the Pity, The (Chagrin et la pitié,...</td>\n      <td>Fat City</td>\n      <td>Au Hasard Balthazar</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                 1   \\\nuser history           Pulp Fiction   \ninitial recs   Pier, The (La Jetée)   \n+ fantasy     2001: A Space Odyssey   \n- fantasy      Pier, The (La Jetée)   \n\n                                                            2   \\\nuser history  Battleship Potemkin, The (Bronenosets Potyomkin)   \ninitial recs                             2001: A Space Odyssey   \n+ fantasy                                 Pier, The (La Jetée)   \n- fantasy                        Sunrise: A Song of Two Humans   \n\n                                        3   \\\nuser history  Woman Under the Influence, A   \ninitial recs                         8 1/2   \n+ fantasy                            8 1/2   \n- fantasy            2001: A Space Odyssey   \n\n                                                             4   \\\nuser history                                     Apocalypse Now   \ninitial recs                      Sunrise: A Song of Two Humans   \n+ fantasy     Dr. Strangelove or: How I Learned to Stop Worr...   \n- fantasy                                                 8 1/2   \n\n                                                5   \\\nuser history  Seven Samurai (Shichinin no samurai)   \ninitial recs        Andrei Rublev (Andrey Rublyov)   \n+ fantasy                          Wild Bunch, The   \n- fantasy                                    Gerry   \n\n                                                    6                   7   \\\nuser history  Wings of Desire (Der Himmel über Berlin)  Son, The (Le Fils)   \ninitial recs                                     Gerry       Paranoid Park   \n+ fantasy                                Touch of Evil           Chinatown   \n- fantasy               Andrei Rublev (Andrey Rublyov)       Paranoid Park   \n\n                                                             8   \\\nuser history                                           Stroszek   \ninitial recs                                    Wild Bunch, The   \n+ fantasy                                           City Lights   \n- fantasy     Sorrow and the Pity, The (Chagrin et la pitié,...   \n\n                                                       9   \\\nuser history                                    Rio Bravo   \ninitial recs                                    Red River   \n+ fantasy     Beauty and the Beast (Belle et la bête, La)   \n- fantasy                                        Fat City   \n\n                                                          10  \nuser history  Wallace & Gromit: The Curse of the Were-Rabbit  \ninitial recs                                 Cyclo (Xich lo)  \n+ fantasy                     Andrei Rublev (Andrey Rublyov)  \n- fantasy                                Au Hasard Balthazar  "
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(outputs).set_index(0)\n",
    "df.index.name = None\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.59178778]])"
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.T@keyphrase_embeddings[18][:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', 'City Lights (1931)', 'Chinatown (1974)', '2001: A Space Odyssey (1968)', 'Godfather, The (1972)', 'Maltese Falcon, The (1941)', 'Annie Hall (1977)', 'Godfather: Part II, The (1974)', 'Double Indemnity (1944)', 'Treasure of the Sierra Madre, The (1948)']\n"
    }
   ],
   "source": [
    "x = keyphrase_embeddings[37][:,np.newaxis]\n",
    "y = [[2]]\n",
    "\n",
    "cov2, mu2 = update_posterior(x, y, cov1, mu1, np.array(np.linalg.norm(1/(cov1+1e-6))))\n",
    "sorted_items, sorted_ratings = get_user_preds_using_mu(mu2, dataset.train_matrix, model)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_title_dict[item_id])\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Pulp Fiction (1994)', 'Star Trek II: The Wrath of Khan (1982)', '8 1/2 (1963)', 'Sling Blade (1996)', 'Dogma (1999)', 'Fight Club (1999)', 'Third Man, The (1949)', 'Lord of the Rings: The Fellowship of the Ring, The (2001)', 'Boat, The (Das Boot) (1981)', 'Fanny and Alexander (Fanny och Alexander) (1982)']\n['Godfather, The (1972)', 'Memento (2000)', 'Usual Suspects, The (1995)', 'Face in the Crowd, A (1957)', 'Saving Private Ryan (1998)', 'Mongol (2007)', 'Bad Santa (2003)', 'Terminator 2: Judgment Day (1991)', 'Curious Case of Benjamin Button, The (2008)', 'Aliens (1986)']\n"
    }
   ],
   "source": [
    "user_id = 77\n",
    "sorted_items, sorted_ratings = get_user_info(user_id, dataset.train_matrix)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_title_dict[item_id])\n",
    "print(output)\n",
    "\n",
    "sorted_items, sorted_ratings = get_user_preds(user_id, dataset.train_matrix)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_title_dict[item_id])\n",
    "print(output)\n",
    "\n",
    "mu, cov = get_mu_cov(user_id, dataset.train_matrix, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Usual Suspects, The (1995)', 'Memento (2000)', 'Terminator 2: Judgment Day (1991)', 'Aliens (1986)', 'Saving Private Ryan (1998)', 'Godfather, The (1972)', 'X2: X-Men United (2003)', 'Gattaca (1997)', 'Spider-Man 2 (2004)', 'Monty Python and the Holy Grail (1975)']\n"
    }
   ],
   "source": [
    "x = keyphrase_embeddings[19][:,np.newaxis]\n",
    "y = [[5.]]\n",
    "\n",
    "cov1, mu1 = update_posterior(x, y, cov, mu, np.array(100))\n",
    "cov1\n",
    "sorted_items, sorted_ratings = get_user_preds_using_mu(mu1, dataset.train_matrix, model)\n",
    "top10_items = sorted_items[:10]\n",
    "output = []\n",
    "for item_id in top10_items:\n",
    "    output.append(id_title_dict[item_id])\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'anime',\n 1: 'super-hero',\n 2: 'woody allen',\n 3: 'comic book',\n 4: 'samuel l jackson',\n 5: 'brad pitt',\n 6: 'bruce willis',\n 7: 'time travel',\n 8: 'crime',\n 9: 'serial killer',\n 10: 'action',\n 11: 'jim carrey',\n 12: 'stupid',\n 13: 'dystopia',\n 14: 'drama',\n 15: 'aliens',\n 16: 'based on a book',\n 17: 'classic',\n 18: 'fantasy',\n 19: 'sci-fi',\n 20: 'space',\n 21: 'arnold schwarzenegger',\n 22: 'matt damon',\n 23: 'oscar (best actor)',\n 24: 'oscar (best picture)',\n 25: 'ghosts',\n 26: 'twist ending',\n 27: 'stephen king',\n 28: 'tom hanks',\n 29: 'interesting',\n 30: 'dvd',\n 31: 'nicolas cage',\n 32: 'magic',\n 33: 'morgan freeman',\n 34: 'bond',\n 35: 'music',\n 36: 'gay',\n 37: 'comedy',\n 38: 'funny',\n 39: 'alfred hitchcock',\n 40: 'true story',\n 41: 'family',\n 42: 'film noir',\n 43: 'heist',\n 44: 'quirky',\n 45: 'sequel',\n 46: 'memory',\n 47: 'psychology',\n 48: 'espionage',\n 49: 'satire',\n 50: 'shakespeare',\n 51: 'black comedy',\n 52: 'adventure',\n 53: 'girlie movie',\n 54: 'teen',\n 55: 'disney',\n 56: 'british',\n 57: 'sports',\n 58: 'hitchcock',\n 59: 'japan',\n 60: 'depressing',\n 61: 'great movie',\n 62: 'overrated',\n 63: 'robin williams',\n 64: 'racism',\n 65: 'boring',\n 66: 'dark comedy',\n 67: 'chick flick',\n 68: 'religion',\n 69: 'holocaust',\n 70: 'world war ii',\n 71: 'mental illness',\n 72: 'martin scorsese',\n 73: 'tom cruise',\n 74: 'steven spielberg',\n 75: 'superhero',\n 76: 'nudity (topless)',\n 77: 'drugs',\n 78: 'mystery',\n 79: 'nonlinear',\n 80: 'revenge',\n 81: 'documentary',\n 82: 'nudity (topless - brief)',\n 83: 'clint eastwood',\n 84: 'england',\n 85: 'zombies',\n 86: 'jude law',\n 87: 'hilarious',\n 88: '007',\n 89: 'james bond',\n 90: 'post-apocalyptic',\n 91: 'based on a true story',\n 92: 'vampire',\n 93: 'politics',\n 94: 'marvel',\n 95: 'animation',\n 96: 'parody',\n 97: 'police',\n 98: 'suicide',\n 99: 'historical',\n 100: 'scary',\n 101: 'fun',\n 102: 'love',\n 103: 'vampires',\n 104: 'murder',\n 105: 'thriller',\n 106: 'horror',\n 107: 'johnny depp',\n 108: 'surreal',\n 109: 'musical',\n 110: 'mel gibson',\n 111: 'remake',\n 112: 'sean connery',\n 113: 'romance',\n 114: 'violence',\n 115: 'london',\n 116: 'war',\n 117: 'martial arts',\n 118: 'lesbian',\n 119: 'death',\n 120: 'black and white',\n 121: 'cute',\n 122: 'seen more than once',\n 123: 'dark',\n 124: 'beautiful',\n 125: 'sad',\n 126: 'history',\n 127: 'disturbing',\n 128: 'children',\n 129: 'france',\n 130: 'terrorism',\n 131: 'fairy tale',\n 132: 'spoof',\n 133: 'paris',\n 134: 'cult film',\n 135: 'prison',\n 136: 'mafia',\n 137: 'organized crime',\n 138: 'predictable',\n 139: 'new york city',\n 140: 'great soundtrack',\n 141: 'underrated',\n 142: 'suspense',\n 143: 'baseball',\n 144: 'conspiracy',\n 145: 'cannibalism',\n 146: 'robots',\n 147: 'high school',\n 148: 'christmas',\n 149: 'nazis',\n 150: 'harrison ford',\n 151: 'coming of age',\n 152: 'epic',\n 153: 'new york',\n 154: 'post apocalyptic',\n 155: 'intense',\n 156: 'violent',\n 157: 'slow',\n 158: 'africa',\n 159: 'future',\n 160: 'biography',\n 161: 'boxing',\n 162: 'los angeles',\n 163: 'cars'}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.6065, 0.6066, 0.6065, 0.6909, 0.6065, 1.0357, 0.6321, 0.7306, 0.8367,\n         0.9005, 0.6890, 0.6065, 0.6202, 0.6615, 0.6069, 0.6066, 0.7204, 0.6065,\n         0.8023, 0.6065, 0.6065, 0.6065, 0.7750, 0.6070, 0.6065, 0.6065, 0.6066,\n         0.6558, 1.0224, 0.6065, 0.6390, 0.7475, 0.6065, 0.6097, 0.6065, 0.6976,\n         0.6065, 0.7138, 0.9611, 0.6307, 0.6065, 0.6065, 0.6065, 0.6065, 0.7756,\n         0.6185, 0.6075, 0.6081, 0.6065, 0.6065]])"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logvar2std(logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1782, 4270, 3084, ..., 1858, 2873, 7813], dtype=int32)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ratings = ratings.argsort()[::-1]\n",
    "sorted_items = items[sorted_ratings]\n",
    "sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1115, 2345, 1815, ..., 1158, 1709, 3787])"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([5., 5., 5., ..., 1., 1., 1.])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[arr1inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =np.random.normal(np.zeros(150),0.01)\n",
    "a = a[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.013279970142235526"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(a.T@(keyphrase_embeddings.T),q=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.00175503, -0.0018365 , -0.00092557, -0.0013183 ,  0.00070177,\n       -0.00045205, -0.00063023,  0.00231926,  0.00274795, -0.00030118,\n       -0.00102944,  0.00221119, -0.00020627,  0.00062257,  0.00136277,\n        0.00194146, -0.00048117, -0.00046926,  0.00112981, -0.00067896,\n        0.00059334,  0.00093145, -0.00020276, -0.00019905, -0.00110767,\n        0.00229955,  0.00026535, -0.00072867, -0.00150996, -0.00102973,\n        0.00242328,  0.00081131,  0.00183528, -0.00051625, -0.00056769,\n       -0.00143513,  0.00020218, -0.00069088, -0.00053919,  0.00166264,\n        0.00039374,  0.00120685, -0.0008378 ,  0.00058671, -0.00062085,\n       -0.00133637,  0.00044003,  0.0004339 , -0.00023481, -0.00046481])"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(np.zeros(50),0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "12.24744871391589"
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((150,150), int)\n",
    "np.fill_diagonal(a, 1)\n",
    "\n",
    "np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictive_dist(x_pred, S, m, prec_y):\n",
    "    print(m.T)\n",
    "    pred_means = m.T @ x_pred\n",
    "    pred_means = pred_means.flatten()\n",
    "    pred_vars = np.sum(1/prec_y + x_pred.T @ S * x_pred.T, axis=1)\n",
    "\n",
    "    return pred_vars, pred_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1 2 3]]\n"
    },
    {
     "data": {
      "text/plain": "(array([39., 84.]), array([14, 20]))"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred = np.array([[1,2,3],[2,3,4]]).T\n",
    "S = np.array([[1,1,1],[1,1,1],[1,1,1]])\n",
    "m = np.array([[1],[2],[3]])\n",
    "get_predictive_dist(x_pred, S, m, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True, False, False])"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "8.999982000027002"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.linalg.norm(1/(S+1e-6)))**2"
   ]
  }
 ]
}